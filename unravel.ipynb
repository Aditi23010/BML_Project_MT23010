{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "from joblib import load \n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import fetch_openml\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column, categorical_columns=[], n_train=None):\n",
    "    # Remove rows where target contains NaN values\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    \n",
    "    # Frequency encoding for categorical features\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    # Split the dataset into features (X) and target (y)\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    y = df[target_column]  # Target\n",
    "\n",
    "    # Get number of features (p) and total samples (n_total)\n",
    "    p = X.shape[1]\n",
    "    n_total = X.shape[0]\n",
    "\n",
    "    # Train-test split\n",
    "    if n_train:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, random_state=42)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, n_total, p, X.columns.tolist()  # Return feature names\n",
    "\n",
    "parkinsons_df = pd.read_csv('data/parkinsons.data')\n",
    "parkinsons_df = parkinsons_df.drop('name', axis=1)\n",
    "target_column = 'status'\n",
    "categorical_columns = []\n",
    "X_train_parkinsons, X_test_parkinsons, y_train_parkinsons, y_test_parkinsons, n_total_parkinsons, p_parkinsons, feature_names_parkinsons = preprocess_data(parkinsons_df, 'status', [], n_train=175)\n",
    "\n",
    "cancer_df = pd.read_csv('data/cancer.csv')\n",
    "cancer_df = cancer_df.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "cancer_df['diagnosis'] = cancer_df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "target_column = 'diagnosis'\n",
    "# Updated unpacking for the Cancer dataset\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer, n_total_cancer, p_cancer, feature_names_cancer = preprocess_data(cancer_df, 'diagnosis', [], n_train=512)\n",
    "\n",
    "\n",
    "adult_df = pd.read_csv('data/adult.csv')\n",
    "adult_df = adult_df.replace('?', np.nan)  # Handle missing values\n",
    "adult_df = adult_df.dropna()  # Drop any rows with missing values\n",
    "categorical_columns = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "target_column = 'income'\n",
    "adult_df[target_column] = adult_df[target_column].map({'<=50K': 0, '>50K': 1})\n",
    "X_train_adult, X_test_adult, y_train_adult, y_test_adult, n_total_adult, p_adult, feature_names_adult = preprocess_data(adult_df, target_column, categorical_columns)\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "X, y = boston.data, boston.target\n",
    "boston_df = X.copy()\n",
    "boston_df['MEDV'] = y  # MEDV is the house price (target)\n",
    "target_column = 'MEDV'\n",
    "categorical_columns = []\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston, n_total_boston, p_boston, feature_names_boston = preprocess_data(boston_df, 'MEDV', [], n_train=455)\n",
    "bodyfat_df = pd.read_csv('data/bodyfat.csv')\n",
    "target_column = 'BodyFat'\n",
    "X_train_bodyfat, X_test_bodyfat, y_train_bodyfat, y_test_bodyfat, n_total_bodyfat, p_bodyfat, feature_names_bodyfat = preprocess_data(bodyfat_df, 'BodyFat', [], n_train=226)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_distance(X_i, X_j):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard distance between two sets of features.\n",
    "    X_i and X_j are expected to be lists of top features.\n",
    "    \"\"\"\n",
    "    set_i = set(X_i)\n",
    "    set_j = set(X_j)\n",
    "    \n",
    "    intersection = len(set_i.intersection(set_j))\n",
    "    union = len(set_i.union(set_j))\n",
    "    \n",
    "    if union == 0:\n",
    "        return 0.0  # If both sets are empty\n",
    "    return 1 - (intersection / union)  # Jaccard distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stability_score(D_final, n_samples=10, n_iterations=50):\n",
    "    \"\"\"\n",
    "    Calculate stability score over n_samples and n_iterations.\n",
    "    D_final is the surrogate data returned by unravel.\n",
    "    \"\"\"\n",
    "    all_explanations = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        current_explanations = []\n",
    "        for sample in random.sample(D_final, n_samples):\n",
    "            # Assume that we have a method to get top-5 features based on the model predictions\n",
    "            top_features = get_top_features(sample[0])  # Replace with actual feature extraction logic\n",
    "            current_explanations.append(top_features)\n",
    "        \n",
    "        all_explanations.append(current_explanations)\n",
    "\n",
    "    # Calculate Jaccard distance for all combinations\n",
    "    jaccard_distances = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        for j in range(i + 1, n_iterations):\n",
    "            for k in range(n_samples):\n",
    "                jaccard_distances.append(calculate_jaccard_distance(all_explanations[i][k], all_explanations[j][k]))\n",
    "\n",
    "    # Average the Jaccard distances\n",
    "    avg_stability = np.mean(jaccard_distances)\n",
    "    return avg_stability\n",
    "\n",
    "def get_top_features(instance):\n",
    "    \"\"\"\n",
    "    Mock function to extract top-5 features for the instance.\n",
    "    You should replace this with your actual feature importance logic.\n",
    "    \"\"\"\n",
    "    # For demonstration purposes, we randomly select 5 features\n",
    "    return random.sample(range(1, 15), 5)  # Adjust according to actual feature space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gpr():\n",
    "    # Initialize the Gaussian Process Regressor with an ARD kernel\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=10.0, length_scale_bounds=(1e-2, 1e5))\n",
    "    return GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "def acquisition_function(X_sample, model, epsilon=1e-6):\n",
    "    # Compute acquisition function (Expected Improvement)\n",
    "    mean, std = model.predict(X_sample, return_std=True)\n",
    "    return mean + std + epsilon  # Add epsilon to avoid zero improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_acquisition_function(model, exploration_domain, n_samples=100):\n",
    "    # Randomly sample from the exploration domain and optimize acquisition function\n",
    "    # exploration_domain is now just (lower_bound, upper_bound)\n",
    "    lower_bound, upper_bound = exploration_domain\n",
    "    \n",
    "    # Generate random samples based on the dimensionality of x0\n",
    "    dimension = len(lower_bound)  # Assuming lower_bound is an array-like structure\n",
    "    X_random = np.random.uniform(lower_bound, upper_bound, (n_samples, dimension))\n",
    "    \n",
    "    scores = acquisition_function(X_random, model)\n",
    "    best_index = np.argmax(scores)\n",
    "    return X_random[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel(model_path, x0, n_iterations=10):\n",
    "    # Load the pre-trained model\n",
    "    model = load(model_path)\n",
    "    \n",
    "    # Initialize data set D\n",
    "    D = [(x0, model.predict([x0])[0])]  # Initial data point with prediction\n",
    "\n",
    "    # Set exploration domain\n",
    "    sigma_D = np.std([data[1] for data in D])  # Calculate standard deviation from current predictions\n",
    "    exploration_domain = (x0 - sigma_D, x0 + sigma_D)  # Adjusted exploration domain\n",
    "\n",
    "    # Initialize GPR\n",
    "    gpr = init_gpr()\n",
    "\n",
    "    for l in range(n_iterations):\n",
    "        # Prepare data for GPR\n",
    "        X_train, y_train = zip(*D)\n",
    "        gpr.fit(np.array(X_train), np.array(y_train))\n",
    "\n",
    "        # Optimize acquisition function to find the next sample\n",
    "        x_next = optimize_acquisition_function(gpr, exploration_domain, n_samples=100)\n",
    "        \n",
    "        # Obtain new prediction and add to dataset\n",
    "        y_next = model.predict([x_next])[0]\n",
    "        D.append((x_next, y_next))\n",
    "\n",
    "        # Update exploration domain\n",
    "        exploration_domain = (x_next - sigma_D, x_next + sigma_D)\n",
    "\n",
    "    return D  # Return the final surrogate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for each dataset/model\n",
    "datasets = {\n",
    "    'Parkinsons': {\n",
    "        'model_path': 'models/parkinsons_svm_model.pkl',\n",
    "        'initial_instance': np.random.rand(22),  # Adjust based on feature number\n",
    "        'iterations': 10\n",
    "    },\n",
    "    'Cancer': {\n",
    "        'model_path': 'models/cancer_svm_model.pkl',\n",
    "        'initial_instance': np.random.rand(30),  # Adjust based on feature number\n",
    "        'iterations': 10\n",
    "    },\n",
    "    'Adult': {\n",
    "        'model_path': 'models/adult_svm_model.pkl',\n",
    "        'initial_instance': np.random.rand(14),  # Adjust based on feature number\n",
    "        'iterations': 10\n",
    "    },\n",
    "    'Boston': {\n",
    "        'model_path': 'models/boston_extra_trees_model.pkl',\n",
    "        'initial_instance': np.random.rand(13),  # Adjust based on feature number\n",
    "        'iterations': 10\n",
    "    },\n",
    "    'BodyFat': {\n",
    "        'model_path': 'models/bodyfat_extra_trees_model.pkl',\n",
    "        'initial_instance': np.random.rand(14),  # Adjust based on feature number\n",
    "        'iterations': 10\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_scores = {\n",
    "    'Dataset': [],\n",
    "    'Stability Score': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:659: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    }
   ],
   "source": [
    "# Run UnRAvEL for each dataset and collect stability scores\n",
    "for dataset, params in datasets.items():\n",
    "    D_final = unravel(params['model_path'], params['initial_instance'], n_iterations=params['iterations'])\n",
    "    \n",
    "    # Assuming some method to compute stability score here, you might have to implement this\n",
    "    stability_score = np.random.rand()  # Placeholder for the actual stability score computation\n",
    "    stability_scores['Dataset'].append(dataset)\n",
    "    stability_scores['Stability Score'].append(stability_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_df = pd.DataFrame(stability_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Dataset  Stability Score\n",
      "0  Parkinsons         0.296821\n",
      "1      Cancer         0.706339\n",
      "2       Adult         0.052505\n",
      "3      Boston         0.857602\n",
      "4     BodyFat         0.486845\n"
     ]
    }
   ],
   "source": [
    "print(stability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_df_LIME=pd.read_csv('stability_results_LIME.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Parkinson's</th>\n",
       "      <td>0.849006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breast Cancer</th>\n",
       "      <td>0.877186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adult Income</th>\n",
       "      <td>0.466304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Boston</th>\n",
       "      <td>0.796952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Body Fat</th>\n",
       "      <td>0.903673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "Parkinson's    0.849006\n",
       "Breast Cancer  0.877186\n",
       "Adult Income   0.466304\n",
       "Boston         0.796952\n",
       "Body Fat       0.903673"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability_df_LIME=stability_df_LIME.transpose()\n",
    "stability_df_LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNRAVEL-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_lime(model_path, x0, n_iterations=10):\n",
    "    # Load the pre-trained model\n",
    "    model = load(model_path)\n",
    "    \n",
    "    # Initialize LIME Explainer\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data=X_train_parkinsons,  # Use the training data corresponding to the model\n",
    "        feature_names=feature_names_parkinsons,  # Adjust based on the dataset being processed\n",
    "        mode='classification' if 'svm' in model_path else 'regression'\n",
    "    )\n",
    "\n",
    "    # Initialize dataset D for storing explanations\n",
    "    D = []\n",
    "    explanations = []\n",
    "    \n",
    "    # Add the initial instance's explanation\n",
    "    exp = explainer.explain_instance(x0, model.predict, num_features=5)\n",
    "    explanations.append(exp.as_list())  # Store the explanation as a list\n",
    "    D.append((x0, model.predict([x0])[0]))  # Initial data point with prediction\n",
    "\n",
    "    for l in range(n_iterations):\n",
    "        # Perturb the initial instance to create new samples\n",
    "        new_samples = [x0 + np.random.normal(0, 0.1, size=x0.shape) for _ in range(10)]\n",
    "        \n",
    "        for sample in new_samples:\n",
    "            # Get prediction for the new sample\n",
    "            pred = model.predict([sample])[0]\n",
    "            D.append((sample, pred))\n",
    "            \n",
    "            # Get LIME explanation for the new sample\n",
    "            exp = explainer.explain_instance(sample, model.predict, num_features=5)\n",
    "            explanations.append(exp.as_list())\n",
    "\n",
    "    return D, explanations  # Return both surrogate data and explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'parkinsons': {\n",
    "        'model_path': 'models/parkinsons_svm_model.pkl',\n",
    "        'initial_instance': X_test_parkinsons[0],  # Example instance from test set\n",
    "        'iterations': 10,\n",
    "        'X_train': X_train_parkinsons,  # Training data for Parkinson's dataset\n",
    "    },\n",
    "    'cancer': {\n",
    "        'model_path': 'models/cancer_svm_model.pkl',\n",
    "        'initial_instance': X_test_cancer[0],  # Example instance from test set\n",
    "        'iterations': 10,\n",
    "        'X_train': X_train_cancer,  # Training data for Cancer dataset\n",
    "    },\n",
    "    'adult': {\n",
    "        'model_path': 'models/adult_svm_model.pkl',\n",
    "        'initial_instance': X_test_adult[0],  # Example instance from test set\n",
    "        'iterations': 10,\n",
    "        'X_train': X_train_adult,  # Training data for Adult dataset\n",
    "    },\n",
    "    'boston': {\n",
    "        'model_path': 'models/boston_extra_trees_model.pkl',\n",
    "        'initial_instance': X_test_boston[0],  # Example instance from test set\n",
    "        'iterations': 10,\n",
    "        'X_train': X_train_boston,  # Training data for Boston dataset\n",
    "    },\n",
    "    'bodyfat': {\n",
    "        'model_path': 'models/bodyfat_extra_trees_model.pkl',\n",
    "        'initial_instance': X_test_bodyfat[0],  # Example instance from test set\n",
    "        'iterations': 10,\n",
    "        'X_train': X_train_bodyfat,  # Training data for BodyFat dataset\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store models\n",
    "models = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_svm_model(X_train, y_train):\n",
    "    model = SVC(probability=True)  # Enable probability estimates\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM models with probability estimates\n",
    "models['parkinsons'] = train_svm_model(X_train_parkinsons, y_train_parkinsons)\n",
    "models['cancer'] = train_svm_model(X_train_cancer, y_train_cancer)\n",
    "models['adult'] = train_svm_model(X_train_adult, y_train_adult)\n",
    "\n",
    "# Similarly, for Extra Trees models\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "def train_extra_trees_model(X_train, y_train):\n",
    "    model = ExtraTreesRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "models['boston'] = train_extra_trees_model(X_train_boston, y_train_boston)\n",
    "models['bodyfat'] = train_extra_trees_model(X_train_bodyfat, y_train_bodyfat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "def unravel_lime(model_path, x0, X_train, n_iterations=10):\n",
    "    # Load the model\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Initialize LIME Explainer based on the model type\n",
    "    if hasattr(model, \"predict_proba\"):  # Classification model\n",
    "        explainer = LimeTabularExplainer(X_train, feature_names=list(range(X_train.shape[1])), mode='classification')\n",
    "    else:  # Regression model\n",
    "        explainer = LimeTabularExplainer(X_train, feature_names=list(range(X_train.shape[1])), mode='regression')\n",
    "\n",
    "    explanations = []\n",
    "    D = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Add the initial instance's explanation\n",
    "        exp = explainer.explain_instance(x0, model.predict, num_features=5)  # Use predict for regression\n",
    "        explanations.append(exp.as_list())  # Store the explanation as a list\n",
    "        D.append((x0, model.predict([x0])[0]))  # Initial data point with prediction\n",
    "\n",
    "    return D, explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stability_scores_lime = {\n",
    "    'Dataset': [],\n",
    "    'Stability Score': []\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_distance(X_i, X_j):\n",
    "    # Convert explanations to sets of feature names (ignoring importance values)\n",
    "    set_i = set([feature for feature, importance in X_i])\n",
    "    set_j = set([feature for feature, importance in X_j])\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = len(set_i & set_j)\n",
    "    union = len(set_i | set_j)\n",
    "    \n",
    "    # Return Jaccard distance\n",
    "    return intersection / union if union > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stability_score(explanations):\n",
    "    total_score = 0\n",
    "    count = 0\n",
    "    n = len(explanations)\n",
    "\n",
    "    # Loop through all pairs of explanations\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            X_i = explanations[i]  # Each element is expected to be a list of tuples\n",
    "            X_j = explanations[j]\n",
    "            score = calculate_jaccard_distance(X_i, X_j)\n",
    "            total_score += score\n",
    "            count += 1\n",
    "\n",
    "    # Return average score\n",
    "    return total_score / count if count > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "LIME does not currently support classifier models without probability scores. If this conflicts with your use case, please let us know: https://github.com/datascienceinc/lime/issues/16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset, params \u001b[38;5;129;01min\u001b[39;00m tqdm(datasets\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m----> 2\u001b[0m     D_final, explanations \u001b[38;5;241m=\u001b[39m \u001b[43munravel_lime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minitial_instance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Calculate stability using the updated function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     stability \u001b[38;5;241m=\u001b[39m calculate_stability_score(explanations)\n",
      "Cell \u001b[1;32mIn[44], line 18\u001b[0m, in \u001b[0;36munravel_lime\u001b[1;34m(model_path, x0, X_train, n_iterations)\u001b[0m\n\u001b[0;32m     14\u001b[0m D \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Add the initial instance's explanation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use predict for regression\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     explanations\u001b[38;5;241m.\u001b[39mappend(exp\u001b[38;5;241m.\u001b[39mas_list())  \u001b[38;5;66;03m# Store the explanation as a list\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     D\u001b[38;5;241m.\u001b[39mappend((x0, model\u001b[38;5;241m.\u001b[39mpredict([x0])[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# Initial data point with prediction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADITI ROY\\Desktop\\BML_PROJECT\\.venv\\Lib\\site-packages\\lime\\lime_tabular.py:361\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(yss\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 361\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIME does not currently support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier models without probability \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores. If this conflicts with your \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse case, please let us know: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m                                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/datascienceinc/lime/issues/16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(yss\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: LIME does not currently support classifier models without probability scores. If this conflicts with your use case, please let us know: https://github.com/datascienceinc/lime/issues/16"
     ]
    }
   ],
   "source": [
    "for dataset, params in tqdm(datasets.items()):\n",
    "    D_final, explanations = unravel_lime(params['model_path'], params['initial_instance'], params['X_train'], n_iterations=params['iterations'])\n",
    "\n",
    "    # Calculate stability using the updated function\n",
    "    stability = calculate_stability_score(explanations)\n",
    "    \n",
    "    # Output the stability score\n",
    "    print(f\"Stability score for {dataset}: {stability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stability_df_lime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstability_df_lime\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stability_df_lime' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(stability_df_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
