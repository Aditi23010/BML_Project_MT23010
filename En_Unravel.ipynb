{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import mean_absolute_error, r2_score,jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column, categorical_columns=[], n_train=None):\n",
    "    # Remove rows where target contains NaN values\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    \n",
    "    # Frequency encoding for categorical features\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    # Split the dataset into features (X) and target (y)\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    y = df[target_column]  # Target\n",
    "\n",
    "    # Get number of features (p) and total samples (n_total)\n",
    "    p = X.shape[1]\n",
    "    n_total = X.shape[0]\n",
    "\n",
    "    # Train-test split\n",
    "    if n_train:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, random_state=42)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, n_total, p, X.columns.tolist()  # Return feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Enhanced Unravel Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedUnRAvEL:\n",
    "    def __init__(self, gp_kernel=None, acquisition_weight=0.5, diversity_lambda=0.3):\n",
    "        # Define the Gaussian Process Kernel with ARD (Automatic Relevance Determination)\n",
    "        if gp_kernel is None:\n",
    "            # Define ARD kernel: C * RBF with automatic relevance determination (ARD)\n",
    "            gp_kernel = C(1.0, (1e-2, 1e2)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "        \n",
    "        # Initialize the Gaussian Process Regressor with the defined kernel\n",
    "        self.surrogate_model = GaussianProcessRegressor(kernel=gp_kernel, normalize_y=True)\n",
    "        # Acquisition weight for combining different acquisition functions\n",
    "        self.acquisition_weight = acquisition_weight\n",
    "        # Lambda for diversity-based penalization\n",
    "        self.diversity_lambda = diversity_lambda\n",
    "        \n",
    "    def inverse_density(self, x, D):\n",
    "        \"\"\"\n",
    "        Correct inverse density calculation.\n",
    "        Higher density means the point x is close to many points in D.\n",
    "        \"\"\"\n",
    "        if len(D) == 0:\n",
    "            return 0\n",
    "        distances = np.linalg.norm(D - x, axis=1) ** 2  # Squared distances\n",
    "        density = 1 / (1 + np.mean(distances))  # Inverse density formula\n",
    "        return density\n",
    "\n",
    "    def adaptive_sampling(self, X_train, X_pool, y_train):\n",
    "        \"\"\"\n",
    "        This method now implements dynamic exploration domain adjustment.\n",
    "        \"\"\"\n",
    "        # Step 1: Calculate uncertainties (exploration) and entropy (exploration)\n",
    "        uncertainties = self.surrogate_model.predict(X_pool, return_std=True)[1]\n",
    "\n",
    "        #Step 2: Simulate entropy sampling (exploration encouragement)\n",
    "        entropy_sampling = np.random.rand(len(X_pool))  # Simulating entropy sampling for exploration\n",
    "\n",
    "        # Step 3: Define dynamic exploration domain width based on density of samples\n",
    "        # Density calculation: higher density areas will lead to smaller exploration domains\n",
    "        #Computing the inverse density for each point in X_pool\n",
    "        densities = np.array([self.inverse_density(x, X_train) for x in X_pool])\n",
    "\n",
    "        #Step 4:Calculate dynamic exploration width based on inverse density\n",
    "        dynamic_width = np.exp(-densities)   # Inverse relationship between density and width\n",
    "\n",
    "        # Step 4: Compute sampling scores using uncertainties, entropy, and dynamic exploration width\n",
    "        sampling_scores = uncertainties * (1 - self.acquisition_weight) + entropy_sampling * self.acquisition_weight\n",
    "        sampling_scores *= dynamic_width  # Adjust by dynamic exploration width\n",
    "\n",
    "        # Return the sample with the highest score\n",
    "        return X_pool[np.argmax(sampling_scores)], sampling_scores[np.argmax(sampling_scores)], uncertainties, entropy_sampling, dynamic_width\n",
    "\n",
    "    def diversity_penalty(self, x, D):\n",
    "        if len(D) == 0:\n",
    "            return 0\n",
    "        return np.mean(np.linalg.norm(D - x, axis=1))\n",
    "\n",
    "    def acquisition_function(self, x, D, uncertainties, entropy_sampling, dynamic_width, x0, sigma, epsilon, n):\n",
    "        \"\"\"\n",
    "        The hybrid acquisition function that combines:\n",
    "        - Faithful Uncertainty Reduction (FUR)\n",
    "        - Diversity Penalization\n",
    "        - Uses adaptive sampling results directly to avoid recalculating scores\n",
    "        \"\"\"\n",
    "        # Faithful Uncertainty Reduction (FUR) - uncertainty-based score\n",
    "        # FUR formula as given in the problem\n",
    "        distance = np.linalg.norm(x - x0)\n",
    "        sigma_n = np.mean(uncertainties)  # Uncertainty at x based on surrogate model\n",
    "\n",
    "        # Calculate the FUR score (exploration + uncertainty)\n",
    "        fur_score = -((distance - sigma * epsilon * np.log(n)) ** 2 / 2) + sigma_n\n",
    "        # Diversity score is based on how similar `x` is to the existing dataset `D`\n",
    "        diversity_score = self.diversity_penalty(x, D)\n",
    "        \n",
    "        # The acquisition score combines FUR and diversity penalty\n",
    "        acquisition_score = fur_score + self.diversity_lambda * diversity_score\n",
    "\n",
    "        # If dynamic width was used to adjust exploration, we adjust acquisition score accordingly\n",
    "        acquisition_score *= np.mean(dynamic_width)  # Adjust the score based on exploration density\n",
    "\n",
    "        return acquisition_score\n",
    "\n",
    "    def train_surrogate(self, X, y):\n",
    "        self.surrogate_model.fit(X, y)\n",
    "\n",
    "    def evaluate_stability(self, top_k_features_1, top_k_features_2):\n",
    "        # Log the top features\n",
    "        print(\"Top-k features ARD:\", top_k_features_1)\n",
    "        print(\"Top-k features LIME:\", top_k_features_2)\n",
    "\n",
    "        # Ensure both arrays have the same size\n",
    "        if len(top_k_features_1) != len(top_k_features_2):\n",
    "            min_len = min(len(top_k_features_1), len(top_k_features_2))\n",
    "            top_k_features_1 = top_k_features_1[:min_len]\n",
    "            top_k_features_2 = top_k_features_2[:min_len]\n",
    "\n",
    "        # Calculate Jaccard distance\n",
    "        intersection = len(set(top_k_features_1) & set(top_k_features_2))\n",
    "        union = len(set(top_k_features_1) | set(top_k_features_2))\n",
    "        jaccard_dist = 1 - (intersection / union) if union != 0 else 1\n",
    "\n",
    "        # Spearman correlation on feature rankings\n",
    "        spearman_corr, _ = spearmanr(top_k_features_1, top_k_features_2)\n",
    "        return jaccard_dist, spearman_corr\n",
    "\n",
    "    def evaluate_fidelity(self, y_true, y_pred):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        return mae, r2\n",
    "\n",
    "    def active_learning_routine(self, X_train, y_train, X_test, y_test, max_iterations=10, top_k=5):\n",
    "        \"\"\"\n",
    "        The active learning routine that trains the surrogate model iteratively and performs sampling.\n",
    "        Incorporates adaptive sampling and acquisition function.\n",
    "        \"\"\"\n",
    "        initial_indices = np.random.choice(len(X_train), size=5, replace=False)\n",
    "        initial_X = X_train[initial_indices]\n",
    "        initial_y = y_train.iloc[initial_indices]\n",
    "\n",
    "        # Train initial surrogate model\n",
    "        self.train_surrogate(initial_X, initial_y)\n",
    "        \n",
    "        dataset_D = initial_X\n",
    "        y_train = np.delete(y_train, initial_indices)\n",
    "        X_train = np.delete(X_train, initial_indices, axis=0)\n",
    "\n",
    "        for iteration in range(max_iterations):\n",
    "            # Adaptive sampling step, returning all necessary scores\n",
    "            x_sample, sampling_score, uncertainties, entropy_sampling, dynamic_width = self.adaptive_sampling(X_train, X_test, y_train)\n",
    "\n",
    "            # Make prediction for the new sample\n",
    "            y_sample_pred = self.surrogate_model.predict(x_sample.reshape(1, -1))\n",
    "            if y_sample_pred.ndim == 0:\n",
    "                y_sample = y_sample_pred\n",
    "            else:\n",
    "                y_sample = y_sample_pred[0]\n",
    "\n",
    "            # Update dataset with new sample and retrain surrogate model\n",
    "            dataset_D = np.vstack([dataset_D, x_sample])\n",
    "            initial_y = np.append(initial_y, y_sample)\n",
    "            self.train_surrogate(dataset_D, initial_y)\n",
    "            \n",
    "            # Use acquisition function with adaptive sampling results directly\n",
    "            acquisition_score = self.acquisition_function(x_sample, dataset_D, uncertainties, entropy_sampling, dynamic_width, initial_X[0], 1.0, np.random.randn(), iteration + 1)\n",
    "            print(f\"Iteration {iteration+1} - Acquisition Score: {acquisition_score}\")\n",
    "            \n",
    "            # Calculate top-k feature importance using ARD and simulate LIME\n",
    "            top_k_features_ard = np.argsort(np.abs(self.surrogate_model.kernel_.theta))[-top_k:]\n",
    "            top_k_features_lime = np.random.choice(X_train.shape[1], top_k, replace=False)  # Simulate LIME results\n",
    "            \n",
    "            # Evaluate stability between ARD and LIME\n",
    "            jaccard_dist, spearman_corr = self.evaluate_stability(top_k_features_ard, top_k_features_lime)\n",
    "            print(f\"Iteration {iteration+1} - Jaccard Distance: {jaccard_dist}, Spearman Correlation: {spearman_corr}\")\n",
    "            \n",
    "            # Evaluate Fidelity (MAE and R^2)\n",
    "            y_pred = self.surrogate_model.predict(X_test)\n",
    "            mae, r2 = self.evaluate_fidelity(y_test, y_pred)\n",
    "            print(f\"Iteration {iteration+1} - MAE: {mae}, R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the enhanced stability metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute stability metric\n",
    "def compute_stability_metric(model, X_test, num_samples=10, top_k=10):\n",
    "    stability_scores_jaccard = []\n",
    "    stability_scores_spearman = []\n",
    "    \n",
    "    # Randomly select num_samples samples\n",
    "    selected_samples = random.sample(range(len(X_test)), num_samples)\n",
    "    \n",
    "    for idx in selected_samples:\n",
    "        # Simulate top k features selection for each method (replace with actual feature selection logic)\n",
    "        top_k_features_model_1 = np.random.choice(X_test.shape[1], top_k, replace=False)  # Simulate model 1\n",
    "        top_k_features_model_2 = np.random.choice(X_test.shape[1], top_k, replace=False)  # Simulate model 2\n",
    "        \n",
    "        # Calculate Jaccard distance\n",
    "        jaccard_dist = 1 - jaccard_score(top_k_features_model_1, top_k_features_model_2, average='macro')\n",
    "        stability_scores_jaccard.append(jaccard_dist)\n",
    "        \n",
    "        # Calculate Spearman correlation\n",
    "        spearman_corr, _ = spearmanr(top_k_features_model_1, top_k_features_model_2)\n",
    "        stability_scores_spearman.append(spearman_corr)\n",
    "    \n",
    "    # Average both Jaccard distance and Spearman correlation\n",
    "    avg_jaccard = np.mean(stability_scores_jaccard)\n",
    "    avg_spearman = np.mean(stability_scores_spearman)\n",
    "    \n",
    "    # Combine both metrics into a final stability score (average of both)\n",
    "    final_stability_metric = (avg_jaccard + avg_spearman) / 2\n",
    "    \n",
    "    return final_stability_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Parkinson's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Parkinson's dataset\n",
    "parkinsons_df = pd.read_csv('data/parkinsons.data')\n",
    "parkinsons_df = parkinsons_df.drop('name', axis=1)\n",
    "target_column = 'status'\n",
    "categorical_columns = []\n",
    "X_train_parkinsons, X_test_parkinsons, y_train_parkinsons, y_test_parkinsons, n_total_parkinsons, p_parkinsons, feature_names_parkinsons = preprocess_data(parkinsons_df, 'status', [], n_train=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Acquisition Score: -16.39266813001795\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 7  0 10  3  6]\n",
      "Iteration 1 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 1 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 2 - Acquisition Score: -22.29951408665141\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [13  9 19  6 10]\n",
      "Iteration 2 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 2 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 3 - Acquisition Score: -13.975839279443642\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 1 17  0  6 15]\n",
      "Iteration 3 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 3 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 4 - Acquisition Score: -10.265165674587495\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 5 15 11  9 16]\n",
      "Iteration 4 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 4 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 5 - Acquisition Score: -31.702492529479667\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 8  3  6  7 19]\n",
      "Iteration 5 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 5 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 6 - Acquisition Score: -7.909972626012806\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 4  2 15  3  6]\n",
      "Iteration 6 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 6 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 7 - Acquisition Score: -29.050733062794755\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [13  7  1  2 16]\n",
      "Iteration 7 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 7 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 8 - Acquisition Score: -13.408731108260389\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [13 12 15 21  2]\n",
      "Iteration 8 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 8 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 9 - Acquisition Score: -52.94603364126699\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [1 0 2 3 7]\n",
      "Iteration 9 - Jaccard Distance: 0.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 9 - MAE: 0.1, R^2: -0.11111111111111116\n",
      "Iteration 10 - Acquisition Score: -25.760075674914447\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [18  8  2  5  4]\n",
      "Iteration 10 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 10 - MAE: 0.1, R^2: -0.11111111111111116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:442: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ADITI ROY\\Desktop\\BML_project\\.venv\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced algorithm\n",
    "algorithm1 = EnhancedUnRAvEL()\n",
    "\n",
    "# Train and evaluate the model on Parkinson's data\n",
    "algorithm1.active_learning_routine(X_train_parkinsons, y_train_parkinsons, X_test_parkinsons, y_test_parkinsons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Metric for Parkinson's dataset: 0.5625541125541126\n"
     ]
    }
   ],
   "source": [
    "# Run the function for Parkinson's data\n",
    "stability_metric_parkinsons = compute_stability_metric(algorithm1, X_test_parkinsons)\n",
    "print(f\"Stability Metric for Parkinson's dataset: {stability_metric_parkinsons}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)Breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.read_csv('data/cancer.csv')\n",
    "cancer_df = cancer_df.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "cancer_df['diagnosis'] = cancer_df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "target_column = 'diagnosis'\n",
    "# Updated unpacking for the Cancer dataset\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer, n_total_cancer, p_cancer, feature_names_cancer = preprocess_data(cancer_df, 'diagnosis', [], n_train=512)\n",
    "# Combine preprocessed data into a DataFrame\n",
    "cancer_train = pd.DataFrame(X_train_cancer)\n",
    "cancer_train['diagnosis'] = y_train_cancer\n",
    "cancer_test = pd.DataFrame(X_test_cancer)\n",
    "cancer_test['diagnosis'] = y_test_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Acquisition Score: -44.8671413725407\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [28  4 29 15  1]\n",
      "Iteration 1 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 1 - MAE: 0.2868452131489382, R^2: 0.2606280680013997\n",
      "Iteration 2 - Acquisition Score: -8.300562162153156\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [15 14  9 28 18]\n",
      "Iteration 2 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 2 - MAE: 0.2926868546239992, R^2: 0.23536494417600362\n",
      "Iteration 3 - Acquisition Score: -22.462878461241218\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [18  7 10 27 12]\n",
      "Iteration 3 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 3 - MAE: 0.3006273692953365, R^2: 0.21920262733889295\n",
      "Iteration 4 - Acquisition Score: -6.462464537422692\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 0  1 27 15 19]\n",
      "Iteration 4 - Jaccard Distance: 0.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 4 - MAE: 0.29992149689855435, R^2: 0.2140821554747636\n",
      "Iteration 5 - Acquisition Score: -10.003867093480208\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [15  4  8 18  0]\n",
      "Iteration 5 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 5 - MAE: 0.30224035129091315, R^2: 0.20381018866264455\n",
      "Iteration 6 - Acquisition Score: -13.39781177611029\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 1  4  6 14 15]\n",
      "Iteration 6 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 6 - MAE: 0.30140577855974743, R^2: 0.20477199762722997\n",
      "Iteration 7 - Acquisition Score: -39.46752247015344\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [17  2 19 13 14]\n",
      "Iteration 7 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 7 - MAE: 0.30386050770775686, R^2: 0.1938404317216178\n",
      "Iteration 8 - Acquisition Score: -1.0151273715520543\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 6 29 15 19 17]\n",
      "Iteration 8 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 8 - MAE: 0.3052841262152982, R^2: 0.1911565170747067\n",
      "Iteration 9 - Acquisition Score: -8.719516171553673\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [22  4  2 12  9]\n",
      "Iteration 9 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 9 - MAE: 0.30474317088506864, R^2: 0.1911883658839486\n",
      "Iteration 10 - Acquisition Score: -2.641744554929387\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [12 26 22 23 29]\n",
      "Iteration 10 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 10 - MAE: 0.30275972084067093, R^2: 0.20584936229833606\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced algorithm\n",
    "algorithm2 = EnhancedUnRAvEL()\n",
    "\n",
    "# Train and evaluate the model on Parkinson's data\n",
    "algorithm2.active_learning_routine(X_train_cancer, y_train_cancer, X_test_cancer, y_test_cancer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Metric for cancer's dataset: 0.5189605614973263\n"
     ]
    }
   ],
   "source": [
    "# Run the function for cancer's data\n",
    "stability_metric_cancer = compute_stability_metric(algorithm2, X_test_cancer)\n",
    "print(f\"Stability Metric for cancer's dataset: {stability_metric_cancer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Adult Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df = pd.read_csv('data/adult.csv')\n",
    "adult_df = adult_df.replace('?', np.nan)  # Handle missing values\n",
    "adult_df = adult_df.dropna()  # Drop any rows with missing values\n",
    "categorical_columns = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "target_column = 'income'\n",
    "adult_df[target_column] = adult_df[target_column].map({'<=50K': 0, '>50K': 1})\n",
    "X_train_adult, X_test_adult, y_train_adult, y_test_adult, n_total_adult, p_adult, feature_names_adult = preprocess_data(adult_df, target_column, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "adult_train = pd.DataFrame(X_train_adult)\n",
    "adult_train['income'] = y_train_adult\n",
    "adult_test = pd.DataFrame(X_test_adult)\n",
    "adult_test['income'] = y_test_adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Acquisition Score: -20.255312444770396\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [12  6  8  1  4]\n",
      "Iteration 1 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 1 - MAE: 0.34917121992470335, R^2: -0.012480871979881458\n",
      "Iteration 2 - Acquisition Score: -26.90951297082103\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 0 11  2 10  5]\n",
      "Iteration 2 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 2 - MAE: 0.3491734126395933, R^2: -0.012459091910198561\n",
      "Iteration 3 - Acquisition Score: -106.57704662114062\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [1 2 8 4 0]\n",
      "Iteration 3 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 3 - MAE: 0.3491687506266019, R^2: -0.012508069258232224\n",
      "Iteration 4 - Acquisition Score: -30.923179800214626\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [5 8 2 0 1]\n",
      "Iteration 4 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 4 - MAE: 0.3491695742316405, R^2: -0.012498627925972894\n",
      "Iteration 5 - Acquisition Score: -29.72151328563664\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 5  7 11  2  4]\n",
      "Iteration 5 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 5 - MAE: 0.3491686663894523, R^2: -0.012509058195873335\n",
      "Iteration 6 - Acquisition Score: -27.803591276599715\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 3  6  7 11  8]\n",
      "Iteration 6 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 6 - MAE: 0.34917831493528567, R^2: -0.01241591312851642\n",
      "Iteration 7 - Acquisition Score: -17.6331939208633\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 1 13  9  7  4]\n",
      "Iteration 7 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 7 - MAE: 0.34917073147265354, R^2: -0.012486000886708482\n",
      "Iteration 8 - Acquisition Score: -12.902635023578297\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 7  2 10  9 13]\n",
      "Iteration 8 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 8 - MAE: 0.34916683952585426, R^2: -0.012531687977953121\n",
      "Iteration 9 - Acquisition Score: -19.222638151837256\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [2 4 9 0 3]\n",
      "Iteration 9 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 9 - MAE: 0.3491646968224241, R^2: -0.012561382347264338\n",
      "Iteration 10 - Acquisition Score: -142.7071373582393\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 4 12 11  1  2]\n",
      "Iteration 10 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 10 - MAE: 0.3491646424107725, R^2: -0.012562182316334214\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced algorithm\n",
    "algorithm3 = EnhancedUnRAvEL()\n",
    "\n",
    "# Train and evaluate the model on adult's data\n",
    "algorithm3.active_learning_routine(X_train_adult, y_train_adult, X_test_adult, y_test_adult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Metric for Adult's dataset: 0.3828812853812854\n"
     ]
    }
   ],
   "source": [
    "# Run the function for Adult's data\n",
    "stability_metric_adult = compute_stability_metric(algorithm3, X_test_adult)\n",
    "print(f\"Stability Metric for Adult's dataset: {stability_metric_adult}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "X, y = boston.data, boston.target\n",
    "boston_df = X.copy()\n",
    "boston_df['MEDV'] = y  # MEDV is the house price (target)\n",
    "target_column = 'MEDV'\n",
    "categorical_columns = []\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston, n_total_boston, p_boston, feature_names_boston = preprocess_data(boston_df, 'MEDV', [], n_train=455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "boston_train = pd.DataFrame(X_train_boston)\n",
    "boston_train['MEDV'] = y_train_boston\n",
    "boston_test = pd.DataFrame(X_test_boston)\n",
    "boston_test['MEDV'] = y_test_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Acquisition Score: 0.026640230416627435\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [10 12  0  6  1]\n",
      "Iteration 1 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 1 - MAE: 5.583870436675114, R^2: -0.01679483163580686\n",
      "Iteration 2 - Acquisition Score: -38.32600212015335\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 4  1  2  0 11]\n",
      "Iteration 2 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 2 - MAE: 5.584043860121499, R^2: -0.016821126112669615\n",
      "Iteration 3 - Acquisition Score: -16.598116015523637\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 0  5  7 11  3]\n",
      "Iteration 3 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 3 - MAE: 6.637840542508282, R^2: -0.14351214813795288\n",
      "Iteration 4 - Acquisition Score: -31.908441676922095\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 3 10 12  0  6]\n",
      "Iteration 4 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 4 - MAE: 6.4066341481783455, R^2: -0.10481750971869142\n",
      "Iteration 5 - Acquisition Score: -12.313149994006018\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [0 2 1 3 4]\n",
      "Iteration 5 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 5 - MAE: 6.132682960585262, R^2: -0.06240957502406941\n",
      "Iteration 6 - Acquisition Score: 0.177870050800074\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 8  5 10  1  6]\n",
      "Iteration 6 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 6 - MAE: 6.137126878088912, R^2: -0.06277164379650646\n",
      "Iteration 7 - Acquisition Score: -6.063376646228279\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 6  7  0  9 12]\n",
      "Iteration 7 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 7 - MAE: 6.164496629329617, R^2: -0.06594153444225226\n",
      "Iteration 8 - Acquisition Score: -21.116109312728856\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [10  6  8  5  4]\n",
      "Iteration 8 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 8 - MAE: 6.32960596762435, R^2: -0.08872113030126183\n",
      "Iteration 9 - Acquisition Score: 1.6425667324271176\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 6  2  1  5 10]\n",
      "Iteration 9 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 9 - MAE: 6.40133592555007, R^2: -0.09946417327635415\n",
      "Iteration 10 - Acquisition Score: -11.041125599727904\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [10  5  9 11 12]\n",
      "Iteration 10 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 10 - MAE: 6.476534960644148, R^2: -0.11112282460967648\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced algorithm\n",
    "algorithm4 = EnhancedUnRAvEL()\n",
    "\n",
    "# Train and evaluate the model on Boston's data\n",
    "algorithm4.active_learning_routine(X_train_boston, y_train_boston, X_test_boston, y_test_boston)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Metric for Boston's dataset: 0.45787296037296044\n"
     ]
    }
   ],
   "source": [
    "# Run the function for Boston's data\n",
    "stability_metric_boston = compute_stability_metric(algorithm3, X_test_boston)\n",
    "print(f\"Stability Metric for Boston's dataset: {stability_metric_boston}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Body fat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyfat_df = pd.read_csv('data/bodyfat.csv')\n",
    "target_column = 'BodyFat'\n",
    "X_train_bodyfat, X_test_bodyfat, y_train_bodyfat, y_test_bodyfat, n_total_bodyfat, p_bodyfat, feature_names_bodyfat = preprocess_data(bodyfat_df, 'BodyFat', [], n_train=226)\n",
    "# Combine preprocessed data into a DataFrame\n",
    "bodyfat_train = pd.DataFrame(X_train_bodyfat)\n",
    "bodyfat_train['BodyFat'] = y_train_bodyfat\n",
    "bodyfat_test = pd.DataFrame(X_test_bodyfat)\n",
    "bodyfat_test['BodyFat'] = y_test_bodyfat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Acquisition Score: -13.583744326419753\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 2  0  3 12  4]\n",
      "Iteration 1 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 1 - MAE: 4.861839445890293, R^2: -0.03384491689245017\n",
      "Iteration 2 - Acquisition Score: -4.1450212446392545\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 1  6  2  9 10]\n",
      "Iteration 2 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 2 - MAE: 4.811315039646255, R^2: -0.01329922968709285\n",
      "Iteration 3 - Acquisition Score: -0.8811939492338686\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [13 12  8 10  7]\n",
      "Iteration 3 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 3 - MAE: 4.908816689591276, R^2: -0.05365089623650432\n",
      "Iteration 4 - Acquisition Score: -2.250010555545999\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [3 6 5 7 8]\n",
      "Iteration 4 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 4 - MAE: 4.9916806914144525, R^2: -0.08408516301254099\n",
      "Iteration 5 - Acquisition Score: -1.1662144158564411\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 7  3  4 13  2]\n",
      "Iteration 5 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 5 - MAE: 5.002066324168401, R^2: -0.09040225654100764\n",
      "Iteration 6 - Acquisition Score: -8.176816968257047\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [10  0  9  4  7]\n",
      "Iteration 6 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 6 - MAE: 5.027189120419586, R^2: -0.0979148893255124\n",
      "Iteration 7 - Acquisition Score: 3.4626381353288274\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [ 9 11  2 12  7]\n",
      "Iteration 7 - Jaccard Distance: 1.0, Spearman Correlation: 0.9999999999999999\n",
      "Iteration 7 - MAE: 4.993269342999792, R^2: -0.08512158189651098\n",
      "Iteration 8 - Acquisition Score: 4.218774201278223\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [12  3  5  0  7]\n",
      "Iteration 8 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 8 - MAE: 4.992016433000318, R^2: -0.08530910544824177\n",
      "Iteration 9 - Acquisition Score: -22.900543154480747\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [3 1 9 0 7]\n",
      "Iteration 9 - Jaccard Distance: 0.6666666666666667, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 9 - MAE: 4.972519178532068, R^2: -0.07410804513124702\n",
      "Iteration 10 - Acquisition Score: -7.466196712306402\n",
      "Top-k features ARD: [0 1]\n",
      "Top-k features LIME: [11  9 10  4  1]\n",
      "Iteration 10 - Jaccard Distance: 1.0, Spearman Correlation: -0.9999999999999999\n",
      "Iteration 10 - MAE: 4.952473776856149, R^2: -0.06630425159092446\n"
     ]
    }
   ],
   "source": [
    "# Initialize the enhanced algorithm\n",
    "algorithm5 = EnhancedUnRAvEL()\n",
    "\n",
    "# Train and evaluate the model on Bodyfat's data\n",
    "algorithm5.active_learning_routine(X_train_bodyfat, y_train_bodyfat, X_test_bodyfat, y_test_bodyfat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Metric for Bodyfat's dataset: 0.39640109890109887\n"
     ]
    }
   ],
   "source": [
    "# Run the function for Bodyfat's data\n",
    "stability_metric_bodyfat = compute_stability_metric(algorithm3, X_test_bodyfat)\n",
    "print(f\"Stability Metric for Bodyfat's dataset: {stability_metric_bodyfat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
