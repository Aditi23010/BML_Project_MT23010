{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from lime import lime_tabular\n",
    "from unravel import core\n",
    "from sklearn.metrics import accuracy_score, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "from scipy.stats import norm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import GPyOpt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import itertools\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(df, target_column, categorical_columns=[], n_train=None):\n",
    "    # Remove rows where target contains NaN values\n",
    "    df = df.dropna(subset=[target_column])\n",
    "    \n",
    "    # Convert categorical features to numeric using OneHotEncoding\n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].astype(str)  # Ensure categorical columns are strings\n",
    "        df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "\n",
    "    # Split the dataset into features (X) and target (y)\n",
    "    X = df.drop(target_column, axis=1)  # Features\n",
    "    y = df[target_column]  # Target\n",
    "\n",
    "    # Train-test split\n",
    "    if n_train:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=n_train, random_state=42)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Update feature names to match the new columns after one-hot encoding\n",
    "    feature_names = X.columns.tolist()  # Get updated feature names after encoding\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, len(X), len(X.columns), feature_names  # Return updated feature names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st dataset- Parkinson's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons_df = pd.read_csv('/home/aditi23010/BML_PROJECT/data/parkinsons.data')\n",
    "parkinsons_df = parkinsons_df.drop('name', axis=1)\n",
    "target_column = 'status'\n",
    "categorical_columns = []\n",
    "X_train_parkinsons, X_test_parkinsons, y_train_parkinsons, y_test_parkinsons, n_total_parkinsons, p_parkinsons, feature_names_parkinsons = preprocess_data(parkinsons_df, 'status', [], n_train=175)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "parkinsons_train = pd.DataFrame(X_train_parkinsons)\n",
    "parkinsons_train['status'] = y_train_parkinsons\n",
    "parkinsons_test = pd.DataFrame(X_test_parkinsons)\n",
    "parkinsons_test['status'] = y_test_parkinsons\n",
    "\n",
    "# Save preprocessed Parkinson's data as CSV\n",
    "parkinsons_train.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/parkinsons_train.csv', index=False)\n",
    "parkinsons_test.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/parkinsons_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Dataset- Breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df = pd.read_csv('/home/aditi23010/BML_PROJECT/data/cancer.csv')\n",
    "cancer_df = cancer_df.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "cancer_df['diagnosis'] = cancer_df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "target_column = 'diagnosis'\n",
    "# Updated unpacking for the Cancer dataset\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer, n_total_cancer, p_cancer, feature_names_cancer = preprocess_data(cancer_df, 'diagnosis', [], n_train=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "cancer_train = pd.DataFrame(X_train_cancer)\n",
    "cancer_train['diagnosis'] = y_train_cancer\n",
    "cancer_test = pd.DataFrame(X_test_cancer)\n",
    "cancer_test['diagnosis'] = y_test_cancer\n",
    "\n",
    "# Save preprocessed Cancer data as CSV\n",
    "cancer_train.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/cancer_train.csv', index=False)\n",
    "cancer_test.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/cancer_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd Dataset - Adult Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adult_df = pd.read_csv('/home/aditi23010/BML_PROJECT/data/adult.csv')\n",
    "adult_df = adult_df.replace('?', np.nan)  # Handle missing values\n",
    "adult_df = adult_df.dropna()  # Drop any rows with missing values\n",
    "categorical_columns = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "target_column = 'income'\n",
    "adult_df[target_column] = adult_df[target_column].map({'<=50K': 0, '>50K': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adult, X_test_adult, y_train_adult, y_test_adult, n_total_adult, p_adult, feature_names_adult = preprocess_data(adult_df, target_column, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "adult_train = pd.DataFrame(X_train_adult)\n",
    "adult_train['income'] = y_train_adult\n",
    "adult_test = pd.DataFrame(X_test_adult)\n",
    "adult_test['income'] = y_test_adult\n",
    "\n",
    "# Save preprocessed Adult data as CSV\n",
    "adult_train.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/adult_train.csv', index=False)\n",
    "adult_test.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/adult_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4th Dataset- Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "X, y = boston.data, boston.target\n",
    "boston_df = X.copy()\n",
    "boston_df['MEDV'] = y  # MEDV is the house price (target)\n",
    "target_column = 'MEDV'\n",
    "categorical_columns = []\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston, n_total_boston, p_boston, feature_names_boston = preprocess_data(boston_df, 'MEDV', [], n_train=455)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "boston_train = pd.DataFrame(X_train_boston)\n",
    "boston_train['MEDV'] = y_train_boston\n",
    "boston_test = pd.DataFrame(X_test_boston)\n",
    "boston_test['MEDV'] = y_test_boston\n",
    "\n",
    "# Save preprocessed Boston data as CSV\n",
    "boston_train.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/boston_train.csv', index=False)\n",
    "boston_test.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/boston_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5th Dataset- Body fat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyfat_df = pd.read_csv('/home/aditi23010/BML_PROJECT/data/bodyfat.csv')\n",
    "target_column = 'BodyFat'\n",
    "X_train_bodyfat, X_test_bodyfat, y_train_bodyfat, y_test_bodyfat, n_total_bodyfat, p_bodyfat, feature_names_bodyfat = preprocess_data(bodyfat_df, 'BodyFat', [], n_train=226)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed data into a DataFrame\n",
    "bodyfat_train = pd.DataFrame(X_train_bodyfat)\n",
    "bodyfat_train['BodyFat'] = y_train_bodyfat\n",
    "bodyfat_test = pd.DataFrame(X_test_bodyfat)\n",
    "bodyfat_test['BodyFat'] = y_test_bodyfat\n",
    "\n",
    "# Save preprocessed Body Fat data as CSV\n",
    "bodyfat_train.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/bodyfat_train.csv', index=False)\n",
    "bodyfat_test.to_csv('/home/aditi23010/BML_PROJECT/preprocessed_data/bodyfat_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed datasets saved successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessed datasets saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training amd Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset      Task p    n_train n_total R²/Accuracy\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate classification models (SVM)\n",
    "def train_svm_classifier(X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM classifier.\n",
    "    :return: accuracy of the model\n",
    "    \"\"\"\n",
    "    # Train SVM classifier\n",
    "    model = SVC(kernel='linear', random_state=42, probability=True)  # Set probability=True for LIME compatibility\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model, f'/home/aditi23010/BML_PROJECT/models/{model_name}_svm_model.pkl')\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Train and evaluate regression models (Extra Trees Regressor)\n",
    "def train_extra_trees_regressor(X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate an Extra Trees Regressor.\n",
    "    :return: R² score of the model\n",
    "    \"\"\"\n",
    "    # Train Extra Trees Regressor\n",
    "    model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model\n",
    "    joblib.dump(model, f'/home/aditi23010/BML_PROJECT/models/{model_name}_extra_trees_model.pkl')\n",
    "\n",
    "    # Predict on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate R² Score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return r2\n",
    "\n",
    "# Print table with dataset information\n",
    "def print_dataset_info(name, task_type, p, n_train, n_total, score):\n",
    "    \"\"\"\n",
    "    Print the dataset information.\n",
    "    :param name: Name of the dataset\n",
    "    :param task_type: 'C' for classification, 'R' for regression\n",
    "    :param p: Number of features\n",
    "    :param n_train: Number of training samples\n",
    "    :param n_total: Total number of samples\n",
    "    :param score: Accuracy for classification or R² score for regression\n",
    "    \"\"\"\n",
    "    print(f\"{name.ljust(12)} {task_type} {str(p).ljust(4)} {str(n_train).ljust(6)} {str(n_total).ljust(7)} {score:.2f}\")\n",
    "\n",
    "# Header for the table\n",
    "print(\"Dataset      Task p    n_train n_total R²/Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if not exists\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and collect scores\n",
    "accuracy_parkinsons = train_svm_classifier(X_train_parkinsons, X_test_parkinsons, y_train_parkinsons, y_test_parkinsons, \"parkinsons\")\n",
    "accuracy_cancer = train_svm_classifier(X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer, \"cancer\")\n",
    "accuracy_adult = train_svm_classifier(X_train_adult, X_test_adult, y_train_adult, y_test_adult, \"adult\")\n",
    "r2_boston = train_extra_trees_regressor(X_train_boston, X_test_boston, y_train_boston, y_test_boston, \"boston\")\n",
    "r2_bodyfat = train_extra_trees_regressor(X_train_bodyfat, X_test_bodyfat, y_train_bodyfat, y_test_bodyfat, \"bodyfat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset   Task p    n_train n_total R²/Accuracy\n",
      "Parkinson's  C 22   175    195     0.85\n",
      "Cancer       C 30   512    569     0.96\n",
      "Adult        C 96   21586  26983   0.85\n",
      "Boston       R 13   455    506     0.92\n",
      "BodyFat      R 14   226    252     1.00\n"
     ]
    }
   ],
   "source": [
    "# Print dataset info\n",
    "print(\"Dataset   Task p    n_train n_total R²/Accuracy\")\n",
    "# Adjust the print statements to use shape[0] for sparse matrices\n",
    "print_dataset_info(\"Parkinson's\", 'C', p_parkinsons, X_train_parkinsons.shape[0], n_total_parkinsons, accuracy_parkinsons)\n",
    "print_dataset_info(\"Cancer\", 'C', p_cancer, X_train_cancer.shape[0], n_total_cancer, accuracy_cancer)\n",
    "print_dataset_info(\"Adult\", 'C', p_adult, X_train_adult.shape[0], n_total_adult, accuracy_adult)  # Adjusted line\n",
    "print_dataset_info(\"Boston\", 'R', p_boston, X_train_boston.shape[0], n_total_boston, r2_boston)\n",
    "print_dataset_info(\"BodyFat\", 'R', p_bodyfat, X_train_bodyfat.shape[0], n_total_bodyfat, r2_bodyfat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation R² score for Body Fat: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_validate_extra_trees(X, y):\n",
    "    \"\"\"\n",
    "    Perform cross-validation on an Extra Trees Regressor.\n",
    "    :return: mean R² score across the folds\n",
    "    \"\"\"\n",
    "    model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Perform 5-fold cross-validation and return the mean R² score\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Cross-validation for Body Fat Dataset\n",
    "cross_val_score_bodyfat = cross_validate_extra_trees(X_train_bodyfat, y_train_bodyfat)\n",
    "print(f\"Cross-validation R² score for Body Fat: {cross_val_score_bodyfat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset   Task p    n_train n_total R²/Accuracy\n",
      "Parkinson's  C 22   175    195     0.85\n",
      "Cancer       C 30   512    569     0.96\n",
      "Adult        C 96   21586  26983   0.85\n",
      "Boston       R 13   455    506     0.92\n",
      "BodyFat      R 14   226    252     1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset   Task p    n_train n_total R²/Accuracy\")\n",
    "# Using shape[0] for sparse matrices to get the number of samples\n",
    "print_dataset_info(\"Parkinson's\", 'C', p_parkinsons, X_train_parkinsons.shape[0], n_total_parkinsons, accuracy_parkinsons)\n",
    "print_dataset_info(\"Cancer\", 'C', p_cancer, X_train_cancer.shape[0], n_total_cancer, accuracy_cancer)\n",
    "print_dataset_info(\"Adult\", 'C', p_adult, X_train_adult.shape[0], n_total_adult, accuracy_adult)  # Adjusted line\n",
    "print_dataset_info(\"Boston\", 'R', p_boston, X_train_boston.shape[0], n_total_boston, r2_boston)\n",
    "print_dataset_info(\"BodyFat\", 'R', p_bodyfat, X_train_bodyfat.shape[0], n_total_bodyfat, r2_bodyfat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Jaccard Index\n",
    "def compute_jaccard_index(set_a, set_b):\n",
    "    intersection = len(set_a.intersection(set_b))\n",
    "    union = len(set_a.union(set_b))\n",
    "    \n",
    "    return 1 - (intersection / union) if union != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lime explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lime_explanations(X_train, y_train, X_test, model, feature_names, num_samples=100, num_iterations=50):\n",
    "    # Ensure X_train is a DataFrame with the correct feature names\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "        X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "        \n",
    "    # Create LIME explainer\n",
    "    explainer = LimeTabularExplainer(X_train.values, feature_names=feature_names, mode='regression')\n",
    "    \n",
    "    # Collect explanations for each iteration\n",
    "    explanation_sets = []\n",
    "    \n",
    "    # Ensure that num_samples does not exceed the available samples\n",
    "    num_samples = min(num_samples, X_test.shape[0])\n",
    "    \n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "        random_indices = np.random.choice(X_test.shape[0], num_samples, replace=False)\n",
    "        explanations = []\n",
    "        for i in random_indices:\n",
    "            exp = explainer.explain_instance(X_test[i], model.predict, num_features=5)  # Use model.predict for regression\n",
    "            feature_names_explained = {feature[0] for feature in exp.as_list()}\n",
    "            explanations.append(feature_names_explained)\n",
    "        explanation_sets.append(explanations)\n",
    "    \n",
    "    # Calculate Jaccard distances between explanations\n",
    "    stability_scores = []\n",
    "    for i in range(num_iterations):\n",
    "        for j in range(i + 1, num_iterations):\n",
    "            for features_i in explanation_sets[i]:\n",
    "                for features_j in explanation_sets[j]:\n",
    "                    jaccard_score = compute_jaccard_index(features_i, features_j)\n",
    "                    stability_scores.append(jaccard_score)\n",
    "\n",
    "    return np.mean(stability_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Jaccard index\n",
    "def compute_jaccard_index(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return 1 - (intersection / union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch the Boston dataset\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "X, y = boston.data, boston.target\n",
    "boston_df = X.copy()\n",
    "boston_df['MEDV'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'MEDV'\n",
    "categorical_columns = []  # No categorical columns in the Boston dataset\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston, n_total_boston, p_boston, feature_names_boston = preprocess_data(boston_df, target_column, categorical_columns, n_train=455)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesRegressor()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesRegressor()\n",
    "model.fit(X_train_boston, y_train_boston)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_index(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return 1 - (intersection / union) if union > 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Parkinson's\": (X_train_parkinsons, y_train_parkinsons, X_test_parkinsons),\n",
    "    \"Breast Cancer\": (X_train_cancer, y_train_cancer, X_test_cancer),\n",
    "    \"Adult Income\": (X_train_adult, y_train_adult, X_test_adult),\n",
    "    \"Boston\": (X_train_boston, y_train_boston, X_test_boston),\n",
    "    \"Body Fat\": (X_train_bodyfat, y_train_bodyfat, X_test_bodyfat)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:53<00:00, 14.28s/it]\n",
      "100%|██████████| 50/50 [50:35<00:00, 60.71s/it]\n",
      "100%|██████████| 50/50 [7:39:41<00:00, 551.64s/it]]\n",
      "100%|██████████| 50/50 [18:54<00:00, 22.68s/it]s/it]\n",
      "100%|██████████| 50/50 [10:15<00:00, 12.30s/it]/it] \n",
      "100%|██████████| 5/5 [9:15:13<00:00, 6662.68s/it]  \n"
     ]
    }
   ],
   "source": [
    "results_LIME = {}\n",
    "\n",
    "# Train and compute LIME scores\n",
    "for name, (X_train, y_train, X_test) in tqdm(datasets.items()):\n",
    "    if name in [\"Parkinson's\", \"Breast Cancer\", \"Adult Income\"]:  # Classification tasks\n",
    "        model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "    else:  # Regression tasks\n",
    "        model = ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate LIME explanations and compute stability score\n",
    "    stability_score = generate_lime_explanations(X_train, y_train, X_test, model, feature_names_parkinsons if name == \"Parkinson's\" else \n",
    "                                                  feature_names_cancer if name == \"Breast Cancer\" else\n",
    "                                                  feature_names_adult if name == \"Adult Income\" else\n",
    "                                                  feature_names_boston if name == \"Boston\" else\n",
    "                                                  feature_names_bodyfat)\n",
    "\n",
    "    results_LIME[name] = stability_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Score for LIME on Parkinson's dataset: 0.8515\n",
      "Stability Score for LIME on Breast Cancer dataset: 0.8768\n",
      "Stability Score for LIME on Adult Income dataset: 0.4683\n",
      "Stability Score for LIME on Boston dataset: 0.7971\n",
      "Stability Score for LIME on Body Fat dataset: 0.9054\n"
     ]
    }
   ],
   "source": [
    "for dataset, score in results_LIME.items():\n",
    "    print(f\"Stability Score for LIME on {dataset} dataset: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/aditi23010/BML_PROJECT/stability_results_LIME.csv\n"
     ]
    }
   ],
   "source": [
    "results_df_LIME = pd.DataFrame([results_LIME])  # Convert dict to DataFrame\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "results_df_LIME.to_csv('/home/aditi23010/BML_PROJECT/stability_results_LIME.csv', index=False)\n",
    "\n",
    "print(\"Results saved to /home/aditi23010/BML_PROJECT/stability_results_LIME.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parkinson's</th>\n",
       "      <th>Breast Cancer</th>\n",
       "      <th>Adult Income</th>\n",
       "      <th>Boston</th>\n",
       "      <th>Body Fat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.85153</td>\n",
       "      <td>0.876756</td>\n",
       "      <td>0.468294</td>\n",
       "      <td>0.797084</td>\n",
       "      <td>0.90536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Parkinson's  Breast Cancer  Adult Income    Boston  Body Fat\n",
       "0      0.85153       0.876756      0.468294  0.797084   0.90536"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayLIME explaination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for generating explanations using BayLIME\n",
    "def generate_baylime_explanations(X_train, y_train, X_test, model, feature_names, num_samples=100, num_iterations=50):\n",
    "    # Ensure X_train is a DataFrame with the correct feature names\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "        X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "    \n",
    "    # Create LIME explainer (BayLIME would modify this with Bayesian interpretation)\n",
    "    explainer = LimeTabularExplainer(X_train.values, feature_names=feature_names, mode='regression')\n",
    "    \n",
    "    # Collect explanations for each iteration\n",
    "    explanation_sets = []\n",
    "    \n",
    "    # Ensure that num_samples does not exceed the available samples\n",
    "    num_samples = min(num_samples, X_test.shape[0])\n",
    "    \n",
    "    for _ in tqdm(range(num_iterations)):\n",
    "        random_indices = np.random.choice(X_test.shape[0], num_samples, replace=False)\n",
    "        explanations = []\n",
    "        for i in random_indices:\n",
    "            # Get explanation using BayesianRidge instead of linear model\n",
    "            exp = explainer.explain_instance(X_test[i], model.predict, num_features=5)  # Use model.predict for regression\n",
    "            feature_names_explained = {feature[0] for feature in exp.as_list()}\n",
    "            explanations.append(feature_names_explained)\n",
    "        explanation_sets.append(explanations)\n",
    "    \n",
    "    # Calculate Jaccard distances between explanations\n",
    "    stability_scores = []\n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        for j in range(i + 1, num_iterations):\n",
    "            for features_i in explanation_sets[i]:\n",
    "                for features_j in explanation_sets[j]:\n",
    "                    jaccard_score = compute_jaccard_index(features_i, features_j)\n",
    "                    stability_scores.append(jaccard_score)\n",
    "\n",
    "    return np.mean(stability_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compute Jaccard index\n",
    "def compute_jaccard_index(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return 1 - (intersection / union) if union > 0 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply BayLIME across datasets\n",
    "def run_baylime_on_datasets(datasets, feature_names_dict):\n",
    "    results_BayLIME = {}\n",
    "    \n",
    "    # Train and compute BayLIME stability score for each dataset\n",
    "    for name, (X_train, y_train, X_test) in tqdm(datasets.items()):\n",
    "        # Using BayesianRidge for regression tasks\n",
    "        model = BayesianRidge()\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Generate BayLIME explanations and compute stability score\n",
    "        stability_score = generate_baylime_explanations(X_train, y_train, X_test, model, feature_names_dict[name])\n",
    "        results_BayLIME[name] = stability_score\n",
    "    \n",
    "    return results_BayLIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets: Using the preprocessed datasets from the previous implementation\n",
    "datasets = {\n",
    "    \"Parkinson's\": (X_train_parkinsons, y_train_parkinsons, X_test_parkinsons),\n",
    "    # \"Breast Cancer\": (X_train_cancer, y_train_cancer, X_test_cancer),\n",
    "    # \"Adult Income\": (X_train_adult, y_train_adult, X_test_adult),\n",
    "    # \"Boston\": (X_train_boston, y_train_boston, X_test_boston),\n",
    "    # \"Body Fat\": (X_train_bodyfat, y_train_bodyfat, X_test_bodyfat)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names for each dataset\n",
    "feature_names_dict = {\n",
    "    \"Parkinson's\": feature_names_parkinsons,\n",
    "    # \"Breast Cancer\": feature_names_cancer,\n",
    "    # \"Adult Income\": feature_names_adult,\n",
    "    # \"Boston\": feature_names_boston,\n",
    "    # \"Body Fat\": feature_names_bodyfat\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:46<00:00, 14.14s/it]\n",
      "100%|██████████| 50/50 [00:00<00:00, 85.53it/s]\n",
      "100%|██████████| 1/1 [11:47<00:00, 707.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Running BayLIME on all datasets\n",
    "results_BayLIME = run_baylime_on_datasets(datasets, feature_names_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Score for BayLIME on Parkinson's dataset: 0.8212\n"
     ]
    }
   ],
   "source": [
    "# Print stability scores for BayLIME on each dataset\n",
    "for dataset, score in results_BayLIME.items():\n",
    "    print(f\"Stability Score for BayLIME on {dataset} dataset: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets: Using the preprocessed datasets from the previous implementation\n",
    "datasets = {\n",
    "    \"Parkinson's\": (X_train_parkinsons, y_train_parkinsons, X_test_parkinsons),\n",
    "    \"Breast Cancer\": (X_train_cancer, y_train_cancer, X_test_cancer),\n",
    "    \"Adult Income\": (X_train_adult, y_train_adult, X_test_adult),\n",
    "    \"Boston\": (X_train_boston, y_train_boston, X_test_boston),\n",
    "    \"Body Fat\": (X_train_bodyfat, y_train_bodyfat, X_test_bodyfat)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names for each dataset\n",
    "feature_names_dict = {\n",
    "    \"Parkinson's\": feature_names_parkinsons,\n",
    "    \"Breast Cancer\": feature_names_cancer,\n",
    "    \"Adult Income\": feature_names_adult,\n",
    "    \"Boston\": feature_names_boston,\n",
    "    \"Body Fat\": feature_names_bodyfat\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [11:39<00:00, 13.98s/it]\n",
      "100%|██████████| 50/50 [00:00<00:00, 92.40it/s] \n",
      "100%|██████████| 50/50 [45:27<00:00, 54.56s/it]\n",
      "100%|██████████| 50/50 [00:04<00:00, 10.17it/s]\n",
      "100%|██████████| 50/50 [4:15:24<00:00, 306.49s/it]\n",
      "100%|██████████| 50/50 [00:12<00:00,  3.93it/s]\n",
      "100%|██████████| 50/50 [15:35<00:00, 18.71s/it]/it]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.95it/s]\n",
      "100%|██████████| 50/50 [08:49<00:00, 10.60s/it]/it]\n",
      "100%|██████████| 50/50 [00:01<00:00, 48.83it/s]\n",
      "100%|██████████| 5/5 [5:37:22<00:00, 4048.44s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Running BayLIME on all datasets\n",
    "results_BayLIME = run_baylime_on_datasets(datasets, feature_names_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stability Score for BayLIME on Parkinson's dataset: 0.8201\n",
      "Stability Score for BayLIME on Breast Cancer dataset: 0.8608\n",
      "Stability Score for BayLIME on Adult Income dataset: 0.5604\n",
      "Stability Score for BayLIME on Boston dataset: 0.8260\n",
      "Stability Score for BayLIME on Body Fat dataset: 0.9085\n"
     ]
    }
   ],
   "source": [
    "# Print stability scores for BayLIME on each dataset\n",
    "for dataset, score in results_BayLIME.items():\n",
    "    print(f\"Stability Score for BayLIME on {dataset} dataset: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/aditi23010/BML_PROJECT/stability_results_BayLIME.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV\n",
    "results_df_BayLIME = pd.DataFrame([results_BayLIME])\n",
    "results_df_BayLIME.to_csv('/home/aditi23010/BML_PROJECT/stability_results_BayLIME.csv', index=False)\n",
    "\n",
    "print(\"Results saved to /home/aditi23010/BML_PROJECT/stability_results_BayLIME.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
